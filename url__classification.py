# -*- coding: utf-8 -*-
"""URL _Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P9b9lwZg0O8Fsbjef182lbDqBMw7Dkoq
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Data

###### importing the required packages and dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

df= pd.read_csv('/content/drive/MyDrive/AI-Project/dataset.csv')

#checking for all the misisng values of columns
df.isna().sum()

"""###### checking for imbalances """

100 * df['Type'].value_counts()/len(df) # this shows there is a 88-12 bias towards type 0 which is the benign class

"""#### EDA Exploratory Data analysis

##### Unique categories under each column
"""

for i in df.select_dtypes(include='object').columns:
  print(f"{i} -> {df[i].nunique()}")

"""###### Charset"""

#to find the different unique categories of charset 
df['CHARSET'].value_counts()

#function to keep the top 5 categories of charset
def CHARSET_CLEANER(x):
  if x not in ['UTF-8','ISO-8859-1','utf-8','us-ascii','iso-8859-1']:
    return "OTHERS"
  else:
    return x

df['CHARSET']=df['CHARSET'].apply(CHARSET_CLEANER)

df['CHARSET'].value_counts()

"""###### Server 

"""

df['SERVER'].value_counts()

#function to keep the top 5 categories of server
def SERVER_CLEANER(x):
  if x not in ['Apache','nginx','Microsoft-HTTPAPI/2.0','None','cloudflare-nginx']:
    return "OTHERS"
  else:
    return x

df['SERVER']=df['SERVER'].apply(SERVER_CLEANER)

df['SERVER'].value_counts()

"""###### WhoIS Statepro"""

df['WHOIS_STATEPRO'].value_counts()

def STATE_CLEANER(x):
    if x not in ['CA','None','NY','WA','Barcelona','FL']:
        return "OTHERS"
    else:
        return x

df['WHOIS_STATEPRO']=df['WHOIS_STATEPRO'].apply(STATE_CLEANER)

df['WHOIS_STATEPRO'].value_counts()

"""###### date cleaner """

def DATE_CLEANER(x):
  if x =='None':
    return "Absent"
  else:
    return "Present"

df['WHOIS_REGDATE']=df['WHOIS_REGDATE'].apply(DATE_CLEANER)

df['WHOIS_UPDATED_DATE']=df['WHOIS_UPDATED_DATE'].apply(DATE_CLEANER)

"""###### dropping url and whois country

"""

df.drop(['URL','WHOIS_COUNTRY'],axis=1,inplace=True)

"""##### EDA heatmap"""

plt.figure(figsize=(20,10))
sns.heatmap(data=df.corr(),cmap='plasma',annot=True)

df2=df.copy()

df2.drop(['CONTENT_LENGTH'],axis=1,inplace=True)

df3=df2.copy()

#create dummy variables, which are binary values for the different attributes

df3=pd.get_dummies(df3,columns=['WHOIS_STATEPRO','WHOIS_REGDATE','CHARSET','SERVER','WHOIS_UPDATED_DATE'],drop_first=True)

df3.head()

df3.isna().sum()

#dropping all the missing values
df3.dropna(inplace=True)

"""##### SMOTE

###### ABOUT SMOTE : Smote is a resampling technique through which we can balance the minority class which is currently imbalanced. It works using a K- nearest neighbours method. For a given sample in the minority class, it finds the K- nearest neighbours and reconstructs the dataset
"""

from imblearn.over_sampling import SMOTE

X=df3.drop("Type",axis=1)
y=df3['Type']

from imblearn.under_sampling import RandomUnderSampler

undersample = RandomUnderSampler(sampling_strategy=0.5)

from imblearn.pipeline import Pipeline

oversample = SMOTE(sampling_strategy=0.5) 
steps = [('o',oversample),('u',undersample)]

pipeline = Pipeline(steps=steps)

X_smote, y_smote=pipeline.fit_resample(X,y)

y_smote.value_counts()

"""# Model : Neural Network """

from sklearn.model_selection import train_test_split

#splitting up the test and train sets 
X_train, X_test, y_train, y_test= train_test_split(X_smote, y_smote, test_size=0.1)

#getting the data in the correct dimension 
X_train_new=np.transpose(X_train)
X_train=X_train_new
X_test_new=np.transpose(X_test)
X_test=X_test_new
y_train_new=y_train.values.reshape(1,2111)
y_train=y_train_new
y_test_new=y_test.values.reshape(1,235)
y_test=y_test_new

#understanding the dataset 
print("The number of training values of X are "+str(X_train.shape))
print("The number of training values of y are "+str(y_train.shape))
print("The number of testing values of X are "+ str(X_test.shape))
print("The number of testing values of y are "+ str(y_test.shape))

#normalizing the array
#norm=np.linalg.norm(X_train)
#X_train=X_train/norm

"""###### helper """

layer_dims=[30,16,8,4,1]

def initialize_parameters_deep(layer_dims):
  np.random.seed(3)
  parameters = {}
  L = len(layer_dims)
  #print("length of layer dims is "+str(L))
  for l in range(1 ,L):
    parameters['W'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
    parameters['b'+str(l)]=np.zeros((layer_dims[l],1))
    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))
    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))
  return parameters

# defining function to be used 
def sigmoid(Z):
  A=1/(1+np.exp(-Z))
  return A 

def relu(x):
    return x * (x > 0)

# backward function to be used 
def der(Z):
  return np.where(Z <= 0, 0, 1)

def relu_backward(dA, activation_cache):
  arr=der(activation_cache)
  dZ=np.multiply(dA, arr)
  return dZ

def sigmoid_backward(dA, activation_cache):
  arr= np.multiply(sigmoid(activation_cache) , (1-sigmoid(activation_cache) ) )
  dZ=np.multiply(dA,arr)
  return dZ

"""###### forward prop"""

#implement the linear part of foward propogation 
def linear_forward(A, W, b):
  Z=np.dot(W,A)+b
  cache=(A,W,b)

  return Z, cache

#linear activation 
def linear_activation_forward(A_prev, W, b, activation):
  if activation=="sigmoid":
    Z,linear_cache=linear_forward(A_prev, W, b)
    A=sigmoid(Z)
    activation_cache=Z
  elif activation =="relu":
    Z, linear_cache=linear_forward(A_prev, W,b)
    A=relu(Z)
    activation_cache=Z
    
  cache=(linear_cache, activation_cache) 
  
  return A, cache

# forward propogation 
def forward_prop(X , parameters):
  caches =[]
  A=X
  L= int(len(parameters)/2)
  #print("value of L is "+str(L))
  for l in range(1,L):
    A_prev=A
    A,cache=linear_activation_forward(A_prev, parameters["W"+str(l)],parameters["b"+str(l)],"relu")
    caches.insert(l,cache)

  AL,cache=linear_activation_forward(A,parameters["W"+str(L)],parameters["b"+str(L)],"sigmoid")
  caches.insert(L,cache)
  return AL, caches

# cost function
def compute_cost(AL,Y):
  m=Y.shape[1]
  cost=(-1/m)*np.sum( np.multiply(Y,np.log(AL)) + np.multiply((1-Y),np.log(1-AL)) )
  cost = np.squeeze(cost)
  return cost

"""###### back prop"""

#linear backward function

def linear_backward(dZ,cache):
  A_prev, W, b = cache
  m = A_prev.shape[1]
  dW=(1/m)*np.dot(dZ,A_prev.T)
  db=(1/m)*np.sum(dZ,axis=1,keepdims=True)
  dA_prev=np.dot(W.T,dZ)

  return dA_prev, dW, db

def linear_activation_backward(dA, cache, activation):
  linear_cache, activation_cache = cache
  if activation == "relu":
    dZ=relu_backward(dA, activation_cache)
    dA_prev,dW,db=linear_backward(dZ,linear_cache)

  elif activation=="sigmoid":
    dZ= sigmoid_backward(dA,activation_cache)
    dA_prev,dW,db=linear_backward(dZ,linear_cache)

  return dA_prev, dW, db

#backward propogation
def back_prop(AL, Y, caches):
  grads = {}
  L = len(caches) # the number of layers
  m = AL.shape[1]
  Y = Y.reshape(AL.shape)
  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
  current_cache=caches[L-1]
  dA_prev_temp,dW_temp,db_temp=linear_activation_backward(dAL,current_cache, activation="sigmoid")
  grads["dA"+str(L-1)]=dA_prev_temp
  grads["dW"+str(L)]=dW_temp
  grads["db"+str(L)]=db_temp

  for l in reversed(range(L-1)):
    current_cache=caches[l]
    dA_prev_temp,dW_temp,db_temp=linear_activation_backward(grads["dA"+str(l+1)],current_cache,activation="relu")
    grads["dA"+str(l)]=dA_prev_temp
    grads["dW"+str(l+1)]=dW_temp
    grads["db"+str(l+1)]=db_temp
  
  return grads

#update parameters 
def update_parameters(params, grads, learning_rate):
  parameters = params.copy()
  L= int(len(parameters)/2)
  for l in range(L):
    parameters["W"+str(l+1)]=parameters["W"+str(l+1)]-grads["dW"+str(l+1)]*learning_rate
    parameters["b"+str(l+1)]=parameters["b"+str(l+1)]-grads["db"+str(l+1)]*learning_rate
  return parameters

"""###### model"""

def model(X,Y, layer_dims,learning_rate=0.0075,num_iter=500,print_cost=True):
  np.random.seed(1)
  costs=[] 
  parameters=initialize_parameters_deep(layer_dims)

  for i in range (0, num_iter):
    AL,caches= forward_prop(X, parameters) #calling forward propogation 
    cost=compute_cost(AL,Y)
    grads=back_prop(AL,Y,caches)
    parameters=update_parameters(parameters,grads,learning_rate)
  
    if print_cost and i % 100 == 0 or i == num_iter - 1:
      print("Cost after iteration {}: {}".format(i, np.squeeze(cost)))
    if i % 100 == 0 or i == num_iter:
      costs.append(cost)
  
  return parameters, costs

"""#Utility functions

##Predict
"""

#prediction 
#1 is for malicious websites and 0 is for benign websites
def predict(X, y, parameters):
  L=len(layer_dims)
  res=X
  for i in range(1,L):
    res=np.dot(parameters['W'+str(i)],res)+parameters['b'+str(i)]
  
  res=np.abs(res)
  for i in range(X.shape[1]) :
    if (res[0][i]>0.5):
      res[0][i]=int(0)
    else:
      res[0][i]=int(1)
    
  success=0
  for i in range (X.shape[1]):
    if(res[0][i]==y[0][i]):
      success=success+1
  
  acc=success/X.shape[1]
  return acc

"""##Train"""

parameters, costs= model(X_train, y_train, layer_dims, num_iter=2000, print_cost=True)

acc_train=predict(X_train, y_train, parameters)
print("The training accuracy is "+str(acc_train))

"""##Test"""

parameters_test, costs_test= model(X_test, y_test, layer_dims, num_iter=2000, print_cost=True)

acc_test=predict(X_test, y_test, parameters_test)
print("the testing accuracy is "+str(acc_test))

"""# Model1 : RandomForest"""

import sklearn.ensemble as ek
from sklearn.linear_model import LogisticRegression

clf1 = ek.RandomForestClassifier(n_estimators=100)

clf1.fit(X_train,y_train)

clf1.score(X_test,y_test)

"""# Model2 : Logistic Regression 

"""

clf2 = LogisticRegression()

clf2.fit(X_train,y_train)

clf2.score(X_test,y_test)

"""# Model3 : GradientBoosting"""

clf3 = ek.GradientBoostingClassifier(n_estimators=100)

clf3.fit(X_train,y_train)

clf3.score(X_test,y_test)